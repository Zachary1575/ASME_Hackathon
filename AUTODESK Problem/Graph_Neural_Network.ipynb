{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c207b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacharyg/opt/anaconda3/envs/research/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0661fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c1cdc",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dec1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(df):\n",
    "    value = df.to_numpy().flatten()\n",
    "    if (value[0] == 'Plastic'):\n",
    "        return torch.tensor([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    elif(value[0] == 'Metal_Ferrous_Steel'):\n",
    "        return torch.tensor([0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    elif(value[0] == 'Metal_Non-Ferrous'):\n",
    "        return torch.tensor([0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    elif(value[0] == 'Metal_Aluminum'):\n",
    "        return torch.tensor([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n",
    "    elif(value[0] == 'Metal_Ferrous'):\n",
    "        return torch.tensor([0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0])\n",
    "    elif(value[0] == 'Wood'):\n",
    "        return torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n",
    "    else:\n",
    "        return torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]) # Other\n",
    "    \n",
    "def one_num(df):\n",
    "    value = df.to_numpy().flatten()\n",
    "    if (value[0] == 'Plastic'):\n",
    "        return torch.tensor(0, dtype=torch.int64)\n",
    "    elif(value[0] == 'Metal_Ferrous_Steel'):\n",
    "        return torch.tensor(1, dtype=torch.int64)\n",
    "    elif(value[0] == 'Metal_Non-Ferrous'):\n",
    "        return torch.tensor(2, dtype=torch.int64)\n",
    "    elif(value[0] == 'Metal_Aluminum'):\n",
    "        return torch.tensor(3, dtype=torch.int64)\n",
    "    elif(value[0] == 'Metal_Ferrous'):\n",
    "        return torch.tensor(4, dtype=torch.int64)\n",
    "    elif(value[0] == 'Wood'):\n",
    "        return torch.tensor(5, dtype=torch.int64)\n",
    "    else:\n",
    "        return torch.tensor(6, dtype=torch.int64) # Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc25887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_dict_body(json_data, key):\n",
    "    \"\"\"\n",
    "    Decomposes the body type for Autodesk dataset\n",
    "    \"\"\"\n",
    "    if (len(json_data[str(key)]) != 0):\n",
    "        part_ids = list(json_data[key].keys())\n",
    "        _data = list(json_data[str(key)].values())\n",
    "        df = pd.DataFrame.from_dict(_data)\n",
    "        df = df.drop(columns=['png', 'smt', 'step', 'obj'])\n",
    "        x = True\n",
    "        data_frame = df\n",
    "        combined_df = df\n",
    "        keys_to_delete = [];\n",
    "        while(x): # Dangerous, also O(n^2) solution\n",
    "            x = False\n",
    "            key_list = data_frame.keys();\n",
    "            for key in key_list:\n",
    "                if (type(data_frame[key].iloc[0]) == type({})):\n",
    "                    keys_to_delete.append(key)\n",
    "                    data_frame = pd.DataFrame.from_dict(data_frame[key].to_dict(), orient=\"index\")\n",
    "                    combined_df = pd.concat([combined_df, data_frame], axis=1) # Concat the pd to master\n",
    "                    x = True\n",
    "                    break\n",
    "        combined_df = combined_df.drop(columns=keys_to_delete)    \n",
    "        combined_df[\"body_id\"] = part_ids\n",
    "        combined_df = combined_df.rename(columns={\n",
    "            \"type\": \"center_of_mass_point_type\", \n",
    "            \"x\": \"center_of_mass_x\", \n",
    "            \"y\": \"center_of_mass_y\",\n",
    "            \"z\": \"center_of_mass_z\",\n",
    "        })\n",
    "        return combined_df\n",
    "    \n",
    "def generate_graph(json_data, file, df):\n",
    "    \"\"\"\n",
    "    Creates Graph: G with networkX\n",
    "    Assumes bodies exist with G\n",
    "    \"\"\"\n",
    "    if ('contacts' not in json_data):\n",
    "        return file\n",
    "    elif (json_data[\"contacts\"] is None):\n",
    "        return file\n",
    "    \n",
    "    df = df.drop(columns=['center_of_mass_point_type', 'name'])\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    contact_set = set()\n",
    "    nodes = []\n",
    "    \n",
    "    for key in list(json_data[\"bodies\"].keys()):\n",
    "        nodes.append(\n",
    "            (key, {'y': one_num(df.loc[df['body_id'] == key].drop(columns=[\n",
    "                            'area', \n",
    "                            'volume', \n",
    "                            'body_id',\n",
    "                            'center_of_mass_x',\n",
    "                            'center_of_mass_y',\n",
    "                            'center_of_mass_z'\n",
    "                        ])),\n",
    "                   'x': torch.tensor(df.loc[df['body_id'] == key].drop(columns=[\n",
    "                       'material_category', \n",
    "                       'body_id'\n",
    "                   ]).to_numpy().flatten(), dtype=torch.float32)\n",
    "                  })\n",
    "        )\n",
    "    \n",
    "    \n",
    "    for contact in json_data[\"contacts\"]:\n",
    "        # We will always assume two entities for each contact point\n",
    "        contact_set.add((contact['entity_one']['body'], contact['entity_two']['body']))\n",
    "    contacts_list = list(contact_set)\n",
    "    \n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(contacts_list)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def unpack_dataset(root_folder = None, exclusion_file_list = {}):\n",
    "    \"\"\"\n",
    "    BFS Implementation of unpacking file directories into a dataset given a root folder.\n",
    "    ** Assumes first surface level unpacking. Assumes only one Assembly.json\n",
    "    \"\"\"\n",
    "    if (not root_folder):\n",
    "        print(\"Invalid root_folder path is None Type.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    num = -1 # Excluding Root folder\n",
    "    assembly_cnt = 0\n",
    "    master_df = pd.DataFrame();\n",
    "    queue = []\n",
    "    \n",
    "    try: \n",
    "        file_count = len(os.listdir(root_folder))\n",
    "    except:\n",
    "        print(\"Invalid root_folder Path:\", root_folder)\n",
    "        return pd.Dataframe()\n",
    "    \n",
    "    # Some extra data collection\n",
    "    graphs = []\n",
    "    graph_order = []\n",
    "    keys = [] \n",
    "    no_contact_files = []\n",
    "    \n",
    "    visited_folders = []\n",
    "    \n",
    "    queue.append(root_folder)\n",
    "    visited_folders.append(root_folder)\n",
    "    \n",
    "    while (len(queue) != 0):\n",
    "        _curr = queue.pop(0)\n",
    "        if (os.path.isdir(_curr)):\n",
    "            os.chdir(_curr);\n",
    "            num += 1;\n",
    "            print(\"Files Processed:\", str(int(num / file_count * 100))+\"% |\", str(num) +  \"/\" + str(file_count), end=\"\\r\")\n",
    "            neighbors = sorted(os.listdir());\n",
    "            for n in neighbors:\n",
    "                if (n not in exclusion_file_list):\n",
    "                    path = _curr + \"/\" + n;\n",
    "                    if (path not in visited_folders and os.path.isdir(n)):\n",
    "                        visited_folders.append(path);\n",
    "                        queue.append(path);\n",
    "                    else:\n",
    "                        if (\"assembly.json\" in path):\n",
    "                            visited_folders.append(path);\n",
    "                            queue.append(path);\n",
    "        else:\n",
    "            assembly_cnt += 1\n",
    "            print(\"Assembly.JSON Processed:\", str(int(assembly_cnt / file_count * 100))+\"% |\", str(assembly_cnt) +  \"/\" + str(file_count), end=\"\\r\")\n",
    "            \n",
    "            try:\n",
    "                # Read Bodies Key\n",
    "                file = open(_curr)\n",
    "                json_data = json.load(file)\n",
    "                keys.append(list(json_data.keys()))\n",
    "                df = decompose_dict_body(json_data, \"bodies\")\n",
    "                master_df = pd.concat([master_df, df], ignore_index=True)\n",
    "                \n",
    "                # Append Graphs \n",
    "                graph = generate_graph(json_data, _curr, df)\n",
    "                if (graph != _curr):\n",
    "                    graph_order.append(_curr)\n",
    "                    graphs.append(graph)\n",
    "                else:\n",
    "                    no_contact_files.append(graph)\n",
    "            except Exception as e:\n",
    "                print(\"Error occured at assembly file:\", _curr)\n",
    "                print(e)\n",
    "            \n",
    "    os.chdir(root_folder) # Return the dir\n",
    "    keys = np.array(flatten(keys))\n",
    "    print(\"Unqiue values:\", np.unique(keys, return_counts=True))\n",
    "    \n",
    "    return master_df, graphs, graph_order\n",
    "\n",
    "hard_drive_path = \"/Volumes/T7/ASME-Hackathon/train_new/Fusion360GalleryDataset_23hackathon_train\"\n",
    "\n",
    "# Annoying, but you have to give the absolute path\n",
    "# df, graphs, graph_order = unpack_dataset(\n",
    "#     hard_drive_path,\n",
    "#     {'.DS_Store'}\n",
    "# )\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bff74c",
   "metadata": {},
   "source": [
    "# Create Graph from Pickle and DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6450d733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zacharyg/Documents/GitHub/ASME_Hackathon/AUTODESK Problem\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './df_main_fe2.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[0;32m----> 2\u001b[0m df_main \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./df_main_fe2.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/research/lib/python3.10/site-packages/pandas/io/pickle.py:187\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    186\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/research/lib/python3.10/site-packages/pandas/io/common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './df_main_fe2.pkl'"
     ]
    }
   ],
   "source": [
    "df_main = pd.read_pickle(\"./df_main_fe2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b86a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(df):\n",
    "    \"\"\"\n",
    "    Creates Graph: G with networkX\n",
    "    Assumes bodies exist with G\n",
    "    \"\"\"    \n",
    "    df = df.drop(columns=['center_of_mass_point_type', 'name'])\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    contact_set = set()\n",
    "    nodes = []\n",
    "    \n",
    "    for key in list(json_data[\"bodies\"].keys()):\n",
    "        nodes.append(\n",
    "            (key, {'y': one_num(df.loc[df['body_id'] == key].drop(columns=[\n",
    "                            'area', \n",
    "                            'volume', \n",
    "                            'body_id',\n",
    "                            'center_of_mass_x',\n",
    "                            'center_of_mass_y',\n",
    "                            'center_of_mass_z'\n",
    "                        ])),\n",
    "                   'x': torch.tensor(df.loc[df['body_id'] == key].drop(columns=[\n",
    "                       'material_category', \n",
    "                       'body_id'\n",
    "                   ]).to_numpy().flatten(), dtype=torch.float32)\n",
    "                  })\n",
    "        )\n",
    "    \n",
    "    \n",
    "    for contact in json_data[\"contacts\"]:\n",
    "        # We will always assume two entities for each contact point\n",
    "        contact_set.add((contact['entity_one']['body'], contact['entity_two']['body']))\n",
    "    contacts_list = list(contact_set)\n",
    "    \n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(contacts_list)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53190806",
   "metadata": {},
   "source": [
    "# Graph Visualization | Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c340a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeGraph(G):\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    node_x = [pos[node][0] for node in G.nodes()]\n",
    "    node_y = [pos[node][1] for node in G.nodes()]\n",
    "\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "\n",
    "    # Create a Scatter trace for nodes\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        text=list(G.nodes()),  # Use node labels as text\n",
    "        textposition='top center',  # Position the text above the nodes\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            showscale=False,\n",
    "            colorscale='Viridis',\n",
    "            size=10,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                title='Node Connections',\n",
    "                xanchor='left',\n",
    "                titleside='right'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create a Scatter trace for Edges\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        line=dict(width=0.5, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "\n",
    "\n",
    "    fig = go.Figure(data=[edge_trace, node_trace])\n",
    "\n",
    "    fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c380ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f492471",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819190a5",
   "metadata": {},
   "source": [
    "# One Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Master_Graph = nx.Graph()\n",
    "\n",
    "for graph_idx in range(4000):\n",
    "    Master_Graph = nx.compose(Master_Graph, graphs[graph_idx])\n",
    "\n",
    "# Remove Isolated Nodes\n",
    "isolated_nodes = [node for node, degree in dict(Master_Graph.degree()).items() if degree == 0]\n",
    "Master_Graph.remove_nodes_from(isolated_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeGraph(Master_Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb9e23",
   "metadata": {},
   "source": [
    "# Pytorch Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d14af",
   "metadata": {},
   "source": [
    "### Conversion[A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_graph = from_networkx(Master_Graph)\n",
    "\n",
    "# Sanity check\n",
    "# print(pyg_graph)\n",
    "print(pyg_graph.x)\n",
    "print(pyg_graph.y)\n",
    "# print(pyg_graph.edge_index)\n",
    "# print(pyg_graph.num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b4be6b",
   "metadata": {},
   "source": [
    "### Conversion [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NetworkX graph and features to PyTorch Geometric data format\n",
    "# edges = np.array(Master_Graph.edges()).T\n",
    "# edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "# print(\"Edges: \\n\\n\", edges)\n",
    "# x = torch.tensor(node_features, dtype=torch.float)\n",
    "# y = torch.tensor([node['club'] for node in graph.nodes.values()], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad7fea7",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbba188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "train_ratio = 0.7\n",
    "num_nodes = pyg_graph.x.shape[0]\n",
    "num_train = int(num_nodes * train_ratio)\n",
    "idx = [i for i in range(num_nodes)]\n",
    "\n",
    "np.random.shuffle(idx)\n",
    "train_mask = torch.full_like(pyg_graph.y, False, dtype=bool)\n",
    "train_mask[idx[:num_train]] = True\n",
    "test_mask = torch.full_like(pyg_graph.y, False, dtype=bool)\n",
    "test_mask[idx[num_train:]] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45650f0",
   "metadata": {},
   "source": [
    "### Load into Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(\n",
    "    x=pyg_graph.x.double(), \n",
    "    edge_index=pyg_graph.edge_index, \n",
    "    y=pyg_graph.y,\n",
    "    num_classes=7,\n",
    "    train_mask=train_mask,\n",
    "    test_mask=test_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2969de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5efc44d9",
   "metadata": {},
   "source": [
    "### GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cc790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(data.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, data.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "model.double()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffce1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(data.x, data.edge_index)\n",
    "visualize(out, color=data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc7de7e",
   "metadata": {},
   "source": [
    "# Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      \n",
    "    return test_acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cf1c6",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3768d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123522ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92c75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d549a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fdef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6ce3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
